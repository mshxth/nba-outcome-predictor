{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smlR9iG-hR7k"
      },
      "source": [
        "# **NBA PREDICTION MODEL**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **INPUT TEAMS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "HOME = \"Boston\"\n",
        "AWAY = \"Oklahoma City\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEAM_TO_ABBR = {\n",
        "    \"Atlanta\": \"ATL\",\n",
        "    \"Boston\": \"BOS\",\n",
        "    \"Brooklyn\": \"BRK\",\n",
        "    \"Charlotte\": \"CHO\",\n",
        "    \"Chicago\": \"CHI\",\n",
        "    \"Cleveland\": \"CLE\",\n",
        "    \"Dallas\": \"DAL\",\n",
        "    \"Denver\": \"DEN\",\n",
        "    \"Detroit\": \"DET\",\n",
        "    \"Golden State\": \"GSW\",\n",
        "    \"Houston\": \"HOU\",\n",
        "    \"Indiana\": \"IND\",\n",
        "    \"LA Clippers\": \"LAC\",\n",
        "    \"LA Lakers\": \"LAL\",\n",
        "    \"Memphis\": \"MEM\",\n",
        "    \"Miami\": \"MIA\",\n",
        "    \"Milwaukee\": \"MIL\",\n",
        "    \"Minnesota\": \"MIN\",\n",
        "    \"New Orleans\": \"NOP\",\n",
        "    \"New York\": \"NYK\",\n",
        "    \"Oklahoma City\": \"OKC\",\n",
        "    \"Orlando\": \"ORL\",\n",
        "    \"Philadelphia\": \"PHI\",\n",
        "    \"Phoenix\": \"PHO\",\n",
        "    \"Portland\": \"POR\",\n",
        "    \"Sacramento\": \"SAC\",\n",
        "    \"San Antonio\": \"SAS\",\n",
        "    \"Toronto\": \"TOR\",\n",
        "    \"Utah\": \"UTA\",\n",
        "    \"Washington\": \"WAS\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **INSTALL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.3)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.32.3)\n",
            "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.5.2)\n",
            "Requirement already satisfied: fake_useragent in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.5.1)\n",
            "Requirement already satisfied: selenium in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.25.0)\n",
            "Requirement already satisfied: webdriver-manager in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.0.2)\n",
            "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/manavsheth/Library/Python/3.12/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: trio~=0.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (0.27.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from webdriver-manager) (1.0.1)\n",
            "Requirement already satisfied: packaging in /Users/manavsheth/Library/Python/3.12/lib/python/site-packages (from webdriver-manager) (24.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (5.28.3)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (75.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/manavsheth/Library/Python/3.12/lib/python/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.6.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/manavsheth/Library/Python/3.12/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pandas requests scikit-learn fake_useragent selenium webdriver-manager tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "import random\n",
        "from fake_useragent import UserAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **BASKETBALL REFERENCE LINKS**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "BOX_SCORES = \"https://www.basketball-reference.com/boxscores/\"\n",
        "SCORES_BY_DATE = BOX_SCORES + \"?month={}&day={}&year={}\"\n",
        "TEAMS = \"https://www.basketball-reference.com/teams/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **SAVE DATA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **GET DATES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_last_7_days():\n",
        "    today = datetime.today()\n",
        "    last_7_days = [(today - timedelta(days=i+1)).strftime('%Y-%m-%d') for i in range(7)]\n",
        "    return last_7_days\n",
        "\n",
        "def get_last_15_days():\n",
        "    today = datetime.today()\n",
        "    last_15_days = [(today - timedelta(days=i+1)).strftime('%Y-%m-%d') for i in range(15)]\n",
        "    return last_15_days\n",
        "\n",
        "def get_last_30_days():\n",
        "    today = datetime.today()\n",
        "    last_30_days = [(today - timedelta(days=i+1)).strftime('%Y-%m-%d') for i in range(30)]\n",
        "    return last_30_days\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **SCORES BY DATES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_score(date):\n",
        "    try:\n",
        "        year, month, day = date.split(\"-\")\n",
        "        url = SCORES_BY_DATE.format(month, day, year)\n",
        "\n",
        "        ua = UserAgent()\n",
        "        headers = {'User-Agent': ua.random}\n",
        "        data = requests.get(url, headers=headers)\n",
        "\n",
        "        soup = BeautifulSoup(data.text, \"html.parser\")\n",
        "\n",
        "        for ad in soup.find_all(\"link\", href=lambda href: href and \"pub.network\" in href):\n",
        "            ad.decompose()\n",
        "        for ad_script in soup.find_all(\"script\", src=lambda src: src and \"pub.network\" in src):\n",
        "            ad_script.decompose()\n",
        "\n",
        "\n",
        "        content = soup.prettify()\n",
        "       \n",
        "        with open(\"SCORES/{}-{}-{}.html\".format(month, day, year), \"w+\") as f:\n",
        "            f.write(content)\n",
        "    \n",
        "    except Exception as e:\n",
        "        tb = traceback.extract_tb(e.__traceback__)\n",
        "        line_number = tb[-1].lineno\n",
        "        print(f\"Exception occurred on line {line_number}: {e}\")\n",
        "\n",
        "def save_last_30_days_scores():\n",
        "    dates = get_last_30_days()\n",
        "    for date in dates:\n",
        "        save_score(date)\n",
        "        time.sleep(3.1)\n",
        "\n",
        "save_last_30_days_scores()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **GAMES BY DATES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_games(date):\n",
        "    try:\n",
        "        year, month, day = date.split(\"-\")\n",
        "        file = \"SCORES/{}-{}-{}.html\".format(month, day, year)\n",
        "        \n",
        "        with open(file) as f:   \n",
        "            page = f.read()\n",
        "    \n",
        "        soup = BeautifulSoup(page, \"html.parser\")\n",
        "        games = soup.find_all('div', class_=\"game_summary expanded nohover\")\n",
        "        \n",
        "        for game in games:            \n",
        "            home_team = game.find('table', class_=\"teams\").find_all('tr')[1].find_all('td')[0].find('a').get_text().strip()\n",
        "            away_team = game.find('table', class_=\"teams\").find_all('tr')[0].find_all('td')[0].find('a').get_text().strip()\n",
        "            \n",
        "            game_url = game.find('td', class_=\"right gamelink\").find('a')['href'][11:]\n",
        "            \n",
        "            url = BOX_SCORES + game_url\n",
        "            \n",
        "            data = requests.get(url)\n",
        "\n",
        "            with open(\"GAMES/{}-{}-{}-{}-{}.html\".format(month, day, year, home_team, away_team), \"w+\") as f:\n",
        "                f.write(data.text)\n",
        "\n",
        "            time.sleep(3.1)\n",
        "                \n",
        "    except Exception as e:\n",
        "        tb = traceback.extract_tb(e.__traceback__)\n",
        "        line_number = tb[-1].lineno\n",
        "        print(f\"Exception occurred on line {line_number}: {e}\")\n",
        "\n",
        "def save_last_30_days_games():\n",
        "    dates = get_last_30_days()\n",
        "    for date in dates:\n",
        "        save_games(date)\n",
        "\n",
        "\n",
        "save_last_30_days_games()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **CURRENT TEAM INFORMATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_team(team):\n",
        "    url = TEAMS + TEAM_TO_ABBR[team] + \"/2025.html\"\n",
        "    data = requests.get(url)\n",
        "\n",
        "    with open(\"TEAMS/{}.html\".format(TEAM_TO_ABBR[team]), \"w+\") as f:\n",
        "        f.write(data.text)\n",
        "\n",
        "for team in TEAM_TO_ABBR.keys():\n",
        "    save_team(team)\n",
        "    time.sleep(3.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **MAPPING DATA FOR RANDOM FOREST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **OFFENSIVE AND DEFENSIVE RATING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [Offensive Rating, Defensive Rating]\n",
        "def scrape_team_ratings_from_game(date, team):\n",
        "    year, month, day = date.split(\"-\")\n",
        "    folder_path = Path('GAMES')\n",
        "\n",
        "    for file_path in folder_path.iterdir():\n",
        "        if file_path.is_file() and file_path.name[:10] == \"{}-{}-{}\".format(month, day, year) and team in file_path.name:\n",
        "            with open(file_path, 'r') as file:\n",
        "                page = file.read()\n",
        "        \n",
        "                soup = BeautifulSoup(page, 'html.parser')\n",
        "                comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
        "    \n",
        "                for comment in comments:\n",
        "                    comment_soup = BeautifulSoup(comment, 'html.parser')\n",
        "                    div_four_factors = comment_soup.find('div', id='div_four_factors')\n",
        "                    if div_four_factors:\n",
        "                        abbr = div_four_factors.find('tbody').find('th').find('a').get_text().strip()\n",
        "                        if (abbr == TEAM_TO_ABBR[team]):\n",
        "                            offensive_rating = float(div_four_factors.find('tbody').find_all('tr')[0].find('td', attrs={'data-stat': 'off_rtg'}).get_text())\n",
        "                            defensive_rating = float(div_four_factors.find('tbody').find_all('tr')[1].find('td', attrs={'data-stat': 'off_rtg'}).get_text())\n",
        "                            return [offensive_rating, defensive_rating]\n",
        "                        else:\n",
        "                            offensive_rating = float(div_four_factors.find('tbody').find_all('tr')[1].find('td', attrs={'data-stat': 'off_rtg'}).get_text())\n",
        "                            defensive_rating = float(div_four_factors.find('tbody').find_all('tr')[0].find('td', attrs={'data-stat': 'off_rtg'}).get_text())\n",
        "                            return [offensive_rating, defensive_rating]      \n",
        "\n",
        "def get_ratings(dates, team):\n",
        "    all_data = []\n",
        "\n",
        "    for date in dates:\n",
        "        game_data = scrape_team_ratings_from_game(date, team)\n",
        "        all_data.append(game_data)\n",
        "    \n",
        "    return all_data\n",
        "\n",
        "def get_avg_ratings(dates, team):\n",
        "    ortg = 0\n",
        "    drtg = 0\n",
        "    count = 0\n",
        "    ratings = get_ratings(dates, team)\n",
        "\n",
        "    for rating in ratings:\n",
        "        if rating:\n",
        "            ortg += rating[0]\n",
        "            drtg += rating[1]\n",
        "            count += 1\n",
        "\n",
        "    ortg /= count\n",
        "    drtg /= count\n",
        "    return [ortg, drtg]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [Offensive Rating, Defensive Rating]\n",
        "def scrape_team_fgp_from_game(date, team):\n",
        "    year, month, day = date.split(\"-\")\n",
        "    folder_path = Path('GAMES')\n",
        "\n",
        "    for file_path in folder_path.iterdir():\n",
        "        if file_path.is_file() and file_path.name[:10] == \"{}-{}-{}\".format(month, day, year) and team in file_path.name:\n",
        "            with open(file_path, 'r') as file:\n",
        "                page = file.read()\n",
        "        \n",
        "                soup = BeautifulSoup(page, 'html.parser')\n",
        "                comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
        "    \n",
        "                for comment in comments:\n",
        "                    comment_soup = BeautifulSoup(comment, 'html.parser')\n",
        "                    div_four_factors = comment_soup.find('div', id='div_four_factors')\n",
        "                    if div_four_factors:\n",
        "                        abbr = div_four_factors.find('tbody').find('th').find('a').get_text().strip()\n",
        "                        if (abbr == TEAM_TO_ABBR[team]):\n",
        "                            fgp = float(div_four_factors.find('tbody').find_all('tr')[0].find('td', attrs={'data-stat': 'efg_pct'}).get_text())\n",
        "                            return fgp\n",
        "                        else:\n",
        "                            fgp = float(div_four_factors.find('tbody').find_all('tr')[1].find('td', attrs={'data-stat': 'efg_pct'}).get_text())\n",
        "                            return fgp\n",
        "                        \n",
        "                 \n",
        "\n",
        "def get_fgps(dates, team):\n",
        "    all_data = []\n",
        "\n",
        "    for date in dates:\n",
        "        game_data = scrape_team_fgp_from_game(date, team)\n",
        "        all_data.append(game_data)\n",
        "    \n",
        "    return all_data\n",
        "\n",
        "def get_avg_fgp(dates, team):\n",
        "    total = 0\n",
        "    count = 0\n",
        "    fgps = get_fgps(dates, team)\n",
        "\n",
        "    for fgp in fgps:\n",
        "        if fgp:\n",
        "            total += fgp\n",
        "            count += 1\n",
        "\n",
        "    avg = total / count\n",
        "    return avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **INJURY RATIO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_team_roster(team):\n",
        "    file = \"TEAMS/{}.html\".format(TEAM_TO_ABBR[team])\n",
        "\n",
        "    with open(file) as f:\n",
        "        page = f.read()\n",
        "    \n",
        "    soup = BeautifulSoup(page, \"html.parser\")\n",
        "\n",
        "    roster = []\n",
        "    roster_data = soup.find('div', id='div_roster').find_all('tr')\n",
        "    for player_data in roster_data:\n",
        "        player = player_data.find('td', attrs={'data-stat': 'player'})\n",
        "        if player:\n",
        "            roster.append(player.find_all('a')[0].get_text())\n",
        "    \n",
        "    return roster\n",
        "\n",
        "def scrape_team_injuries(team):\n",
        "    file = \"TEAMS/{}.html\".format(TEAM_TO_ABBR[team])\n",
        "    \n",
        "    with open(file) as f:\n",
        "        page = f.read()\n",
        "    \n",
        "    soup = BeautifulSoup(page, \"html.parser\")\n",
        "   \n",
        "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
        "    \n",
        "    for comment in comments:\n",
        "        comment_soup = BeautifulSoup(comment, 'html.parser')  \n",
        "        div_injuries = comment_soup.find('div', class_='table_container', id='div_injuries')\n",
        "        if div_injuries:\n",
        "            injuries = div_injuries.find('tbody').find_all('a')\n",
        "            count = -1\n",
        "            reserves = []\n",
        "            for injury in injuries:\n",
        "                count += 1\n",
        "                if injury and count % 2 == 0:\n",
        "                    reserves.append(injury.get_text())\n",
        "            return reserves\n",
        "\n",
        "def scrape_injuries_from_date(date, home, away):\n",
        "    year, month, day = date.split(\"-\")\n",
        "    file = \"GAMES/{}-{}-{}-{}-{}.html\".format(month, day, year, home, away)\n",
        "\n",
        "    with open(file) as f:\n",
        "        page = f.read()\n",
        "\n",
        "    soup = BeautifulSoup(page, \"html.parser\")\n",
        "    inactives = soup.find('strong', text='Inactive:\\xa0').find_parent('div').find_all('a')\n",
        "    injuries = []\n",
        "    for inactive in inactives:\n",
        "        injuries.append(inactive.get_text())\n",
        "    \n",
        "    return injuries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_player_value(ppg, rpg, apg, spg, bpg):\n",
        "    return ppg + (1.2 * rpg) + (1.5 * apg) + (2 * spg) + (2 * bpg)\n",
        "\n",
        "def scrape_team_value(team):\n",
        "    file = \"TEAMS/{}.html\".format(TEAM_TO_ABBR[team])\n",
        "    \n",
        "    with open(file) as f:\n",
        "        page = f.read()\n",
        "\n",
        "    soup = BeautifulSoup(page, \"html.parser\")\n",
        "    players = soup.find('table', id='per_game_stats').find('tbody').find_all('tr')\n",
        "\n",
        "    total = 0\n",
        "\n",
        "    for player in players:\n",
        "        ppg = float(player.find('td', attrs={'data-stat': 'pts_per_g'}).get_text())\n",
        "        rpg = float(player.find('td', attrs={'data-stat': 'trb_per_g'}).get_text())\n",
        "        apg = float(player.find('td', attrs={'data-stat': 'ast_per_g'}).get_text())\n",
        "        spg = float(player.find('td', attrs={'data-stat': 'stl_per_g'}).get_text())\n",
        "        bpg = float(player.find('td', attrs={'data-stat': 'blk_per_g'}).get_text())\n",
        "        total += get_player_value(ppg, rpg, apg, spg, bpg)\n",
        "        \n",
        "    return total\n",
        "\n",
        "def scrape_player_value(team, player_name):\n",
        "    file = \"TEAMS/{}.html\".format(TEAM_TO_ABBR[team])\n",
        "    \n",
        "    with open(file) as f:\n",
        "        page = f.read()\n",
        "\n",
        "    soup = BeautifulSoup(page, \"html.parser\")\n",
        "    players = soup.find('table', id='per_game_stats').find('tbody').find_all('tr')\n",
        "\n",
        "    for player in players:\n",
        "        name = player.find('td', attrs={'data-stat': 'name_display'}).find('a').get_text()\n",
        "        if name == player_name: \n",
        "            ppg = float(player.find('td', attrs={'data-stat': 'pts_per_g'}).get_text())\n",
        "            rpg = float(player.find('td', attrs={'data-stat': 'trb_per_g'}).get_text())\n",
        "            apg = float(player.find('td', attrs={'data-stat': 'ast_per_g'}).get_text())\n",
        "            spg = float(player.find('td', attrs={'data-stat': 'stl_per_g'}).get_text())\n",
        "            bpg = float(player.find('td', attrs={'data-stat': 'blk_per_g'}).get_text())\n",
        "            value =  get_player_value(ppg, rpg, apg, spg, bpg)\n",
        "            return value\n",
        "\n",
        "def get_injury_value(injuries, team):\n",
        "    injury_value = 0\n",
        "    total_value = scrape_team_value(team)\n",
        "    if injuries:\n",
        "        for injury in injuries:\n",
        "            player_value = scrape_player_value(team, injury)\n",
        "            if player_value:\n",
        "                injury_value += player_value\n",
        "    \n",
        "    return (total_value - injury_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_team_advanced(team):\n",
        "    file = \"TEAMS/{}.html\".format(TEAM_TO_ABBR[team])\n",
        "\n",
        "    with open(file) as f:\n",
        "        page = f.read()\n",
        "    \n",
        "    soup = BeautifulSoup(page, \"html.parser\")\n",
        "\n",
        "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
        "    \n",
        "    for comment in comments:\n",
        "        comment_soup = BeautifulSoup(comment, 'html.parser')  \n",
        "        advanced = comment_soup.find('table', id='advanced')\n",
        "        if advanced:\n",
        "            advanced = advanced.find('tbody').find_all('tr')\n",
        "            total = 0\n",
        "            for player in advanced:\n",
        "                vorp = float(player.find('td', attrs={'data-stat': 'vorp'}).get_text())\n",
        "                total += vorp\n",
        "\n",
        "            return total\n",
        "    \n",
        "def scrape_player_advanced(team, player_name):\n",
        "    file = \"TEAMS/{}.html\".format(TEAM_TO_ABBR[team])\n",
        "\n",
        "    with open(file) as f:\n",
        "        page = f.read()\n",
        "    \n",
        "    soup = BeautifulSoup(page, \"html.parser\")\n",
        "\n",
        "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
        "    \n",
        "    for comment in comments:\n",
        "        comment_soup = BeautifulSoup(comment, 'html.parser')  \n",
        "        advanced = comment_soup.find('table', id='advanced')\n",
        "        if advanced:\n",
        "            advanced = advanced.find('tbody').find_all('tr')\n",
        "            for player in advanced:\n",
        "                name = player.find('td', attrs={'data-stat': 'name_display'}).find('a').get_text()\n",
        "                if name == player_name:\n",
        "                    vorp = float(player.find('td', attrs={'data-stat': 'vorp'}).get_text())\n",
        "                    return vorp\n",
        "\n",
        "def get_injury_advanced(injuries, team):\n",
        "    injury_advanced = 0\n",
        "    total_advanced = scrape_team_advanced(team)\n",
        "    if injuries:\n",
        "        for injury in injuries:\n",
        "            player_advanced = scrape_player_advanced(team, injury)\n",
        "            if player_advanced:\n",
        "                injury_advanced += player_advanced\n",
        "    \n",
        "    return (total_advanced - injury_advanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_home_win(date, home, away):\n",
        "    year, month, day = date.split(\"-\")\n",
        "    \n",
        "    file = \"GAMES/{}-{}-{}-{}-{}.html\".format(month, day, year, home, away)\n",
        "    with open(file) as f:\n",
        "        page = f.read()\n",
        "\n",
        "    soup = BeautifulSoup(page, \"html.parser\")\n",
        "    scores = soup.find('div', class_='scorebox').find_all('div', class_='scores')\n",
        "\n",
        "    away_score = float(scores[0].find('div', class_='score').get_text())\n",
        "    home_score = float(scores[1].find('div', class_='score').get_text())\n",
        "\n",
        "    if home_score > away_score:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **ORGANIZE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_game_data_from_date(date, home, away):\n",
        "    home_ratings = scrape_team_ratings_from_game(date, home)\n",
        "    away_ratings = scrape_team_ratings_from_game(date, away)\n",
        "\n",
        "    home_net = home_ratings[0] - home_ratings[1]\n",
        "    away_net = away_ratings[0] - away_ratings[1]\n",
        "\n",
        "    home_fgp = scrape_team_fgp_from_game(date, home)\n",
        "    away_fgp = scrape_team_fgp_from_game(date, away)\n",
        "\n",
        "    injuries = scrape_injuries_from_date(date, home, away)\n",
        "\n",
        "    home_injury_value = get_injury_value(injuries, home)\n",
        "    away_injury_value = get_injury_value(injuries, away)\n",
        "\n",
        "    home_injury_advanced = get_injury_advanced(injuries, home)\n",
        "    away_injury_advanced = get_injury_advanced(injuries, away)\n",
        "\n",
        "\n",
        "    home_win = scrape_home_win(date, home, away)\n",
        "\n",
        "    return [\n",
        "            home, away, \n",
        "            home_net, away_net,\n",
        "            home_fgp, away_fgp,\n",
        "            home_injury_value, away_injury_value,\n",
        "            home_injury_advanced, away_injury_advanced,\n",
        "            home_win\n",
        "            ]\n",
        "\n",
        "def get_all_data():\n",
        "    try:\n",
        "        folder_path = Path('GAMES')\n",
        "        folder_list = list(folder_path.iterdir())\n",
        "        folder_list.sort()\n",
        "        game_data = []\n",
        "        for file_path in folder_list:\n",
        "            if file_path.is_file() and file_path.name != \".DS_Store\":\n",
        "                data = file_path.name.split(\"-\")\n",
        "                date = data[2] + \"-\" + data[0] + \"-\" + data[1]\n",
        "                home = data[3]\n",
        "                away = data[4][:len(data[4]) - 5]\n",
        "                game = get_game_data_from_date(date, home, away)\n",
        "                print(game)\n",
        "                game_data.append(game)\n",
        "        return game_data\n",
        "    \n",
        "    except Exception as e:\n",
        "        tb = traceback.extract_tb(e.__traceback__)\n",
        "        line_number = tb[-1].lineno\n",
        "        print(f\"Exception occurred on line {line_number}: {e}\")\n",
        "        \n",
        "columns = [\n",
        "    \"Home\", \"Away\", \n",
        "    \"Home Net Rating\", \"Away Net Rating\",\n",
        "    \"Home eFG%\", \"Away eFG%\",\n",
        "    \"Home Injury Value\", \"Away Injury Value\",\n",
        "    \"Home Injury Advanced\", \"Away Injury Advanced\",\n",
        "    \"Home Win\"\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(get_all_data(), columns=columns)\n",
        "\n",
        "df = pd.get_dummies(df, columns=['Home', 'Away'])\n",
        "\n",
        "df = df.astype(float)\n",
        "\n",
        "file_path = 'DATA/30_days_data_2.xlsx'\n",
        "df.to_excel(file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **RANDOM FOREST MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ng/17qb206971x8rknwjqdlgc2h0000gn/T/ipykernel_84653/1066898642.py:29: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  comment_soup = BeautifulSoup(comment, 'html.parser')\n",
            "/var/folders/ng/17qb206971x8rknwjqdlgc2h0000gn/T/ipykernel_84653/26041442.py:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  comment_soup = BeautifulSoup(comment, 'html.parser')\n",
            "/var/folders/ng/17qb206971x8rknwjqdlgc2h0000gn/T/ipykernel_84653/26041442.py:34: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  comment_soup = BeautifulSoup(comment, 'html.parser')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 0.6444444444444445\n",
            "Boston will win\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define features and target\n",
        "features = [col for col in df.columns if col != 'Home Win' and col != 'Home' and col != 'Away']\n",
        "target = 'Home Win'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "home_ratings_30_days = get_avg_ratings(get_last_30_days(), HOME)\n",
        "away_ratings_30_days = get_avg_ratings(get_last_30_days(), AWAY)\n",
        "home_net_30_days = home_ratings_30_days[0] - home_ratings_30_days[1]\n",
        "away_net_30_days = away_ratings_30_days[0] - away_ratings_30_days[1]\n",
        "\n",
        "home_ratings_15_days = get_avg_ratings(get_last_15_days(), HOME)\n",
        "away_ratings_15_days = get_avg_ratings(get_last_15_days(), AWAY)\n",
        "home_net_15_days = home_ratings_15_days[0] - home_ratings_15_days[1]\n",
        "away_net_15_days = away_ratings_15_days[0] - away_ratings_15_days[1]\n",
        "\n",
        "home_ratings_7_days = get_avg_ratings(get_last_7_days(), HOME)\n",
        "away_ratings_7_days = get_avg_ratings(get_last_7_days(), AWAY)\n",
        "home_net_7_days = home_ratings_7_days[0] - home_ratings_7_days[1]\n",
        "away_net_7_days = away_ratings_7_days[0] - away_ratings_7_days[1]\n",
        "\n",
        "home_net = (home_net_30_days + home_net_15_days + home_net_7_days) / 3\n",
        "away_net = (away_net_30_days + away_net_15_days + away_net_7_days) / 3\n",
        "\n",
        "home_fgp_30_days =  get_avg_fgp(get_last_30_days(), HOME)\n",
        "away_fgp_30_days =  get_avg_fgp(get_last_30_days(), AWAY)\n",
        "\n",
        "home_fgp_15_days =  get_avg_fgp(get_last_15_days(), HOME)\n",
        "away_fgp_15_days =  get_avg_fgp(get_last_15_days(), AWAY)\n",
        "\n",
        "home_fgp_7_days = get_avg_fgp(get_last_7_days(), HOME)\n",
        "away_fgp_7_days = get_avg_fgp(get_last_7_days(), AWAY)\n",
        "\n",
        "home_fgp = (home_fgp_30_days + home_fgp_15_days + home_fgp_7_days) / 3\n",
        "away_fgp = (away_fgp_30_days + away_fgp_15_days + away_fgp_7_days) / 3\n",
        "\n",
        "home_injuries = scrape_team_injuries(HOME)\n",
        "away_injuries = scrape_team_injuries(AWAY)\n",
        "\n",
        "home_injury_value = get_injury_value(home_injuries, HOME)\n",
        "away_injury_value = get_injury_value(away_injuries, AWAY)\n",
        "\n",
        "home_injury_advanced = get_injury_advanced(home_injuries, HOME)\n",
        "away_injury_advanced = get_injury_advanced(away_injuries, AWAY)\n",
        "\n",
        "\n",
        "input_data = { \n",
        "    \"Home Net Rating\": [home_net], \n",
        "    \"Away Net Rating\": [away_net], \n",
        "    \"Home eFG%\": [home_fgp], \n",
        "    \"Away eFG%\": [away_fgp], \n",
        "    \"Home Injury Value\": [home_injury_value], \n",
        "    \"Away Injury Value\": [away_injury_value],\n",
        "    \"Home Injury Advanced\": [home_injury_advanced], \n",
        "    \"Away Injury Advanced\": [away_injury_advanced],\n",
        "}\n",
        "\n",
        "\n",
        "for col in X.columns:\n",
        "    if col.startswith(\"Home_\") or col.startswith(\"Away_\"):\n",
        "        input_data[col] = [0]\n",
        "\n",
        "home = \"Home_{}\".format(HOME)\n",
        "away = \"Away_{}\".format(AWAY)\n",
        "\n",
        "input_data[home] = [1]\n",
        "input_data[away] = [1]\n",
        "\n",
        "\n",
        "# Convert new game data into a DataFrame\n",
        "prediction_df = pd.DataFrame(input_data)\n",
        "\n",
        "# Predict the outcome for the new game\n",
        "prediction = rf_model.predict(prediction_df)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "# Output the prediction\n",
        "if prediction[0] == 1:\n",
        "    print(HOME + \" will win\")\n",
        "else:\n",
        "    print(AWAY + \" will win\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# Define features and target\\nfeatures = [col for col in df.columns if col != \\'Home Win\\']\\ntarget = \\'Home Win\\'\\n\\nX = df[features]\\ny = df[target]\\n\\n# Split the data\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Define the neural network model\\nmodel = Sequential([\\n    Dense(64, input_dim=X_train.shape[1], activation=\\'relu\\'),  # Input and first hidden layer\\n    Dense(32, activation=\\'relu\\'),  # Second hidden layer\\n    Dense(1, activation=\\'sigmoid\\')  # Output layer (binary classification)\\n])\\n\\n# Compile the model\\nmodel.compile(optimizer=\\'adam\\', loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train the model\\nmodel.fit(X_train, y_train, epochs=100, batch_size=8, verbose=1, validation_split=0.1)\\n\\n# Evaluate the model\\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\\nprint(f\"Test Accuracy: {accuracy:.2f}\")\\n\\n# Predict for a new game\\nhome_ratings_30_days = RATINGS_30_DAY[HOME]\\naway_ratings_30_days = RATINGS_30_DAY[AWAY]\\n\\nhome_ratings_15_days = RATINGS_15_DAY[HOME]\\naway_ratings_15_days = RATINGS_15_DAY[AWAY]\\n\\nhome_ratings_7_days = RATINGS_7_DAY[HOME]\\naway_ratings_7_days = RATINGS_7_DAY[AWAY]\\n\\nhome_injuries = scrape_team_injuries(HOME)\\naway_injuries = scrape_team_injuries(AWAY)\\n\\nhome_injury_value = get_injury_advanced(home_injuries, HOME)\\naway_injury_value = get_injury_advanced(away_injuries, AWAY)\\n\\n\\ninput_data = { \\n    \"30 Day Home Net Rating\": [home_ratings_30_days[0] - home_ratings_30_days[1]], \\n    \"30 Day Away Net Rating\": [away_ratings_30_days[0] - away_ratings_30_days[1]], \\n    \"15 Day Home Net Rating\": [home_ratings_15_days[0] - home_ratings_15_days[1]], \\n    \"15 Day Away Net Rating\": [away_ratings_15_days[0] - away_ratings_15_days[1]],\\n    \"7 Day Home Net Rating\": [home_ratings_7_days[0] - home_ratings_7_days[1]], \\n    \"7 Day Away Net Rating\": [away_ratings_7_days[0] - away_ratings_7_days[1]],\\n    \"Home Injury Advanced\": [home_injury_value], \\n    \"Away Injury Advanced\": [away_injury_value],\\n}\\n\\n\\nfor col in X.columns:\\n    if col.startswith(\"Home_\") or col.startswith(\"Away_\"):\\n        input_data[col] = [0]\\n\\nhome = \"Home_{}\".format(HOME)\\naway = \"Away_{}\".format(AWAY)\\n\\ninput_data[home] = [1]\\ninput_data[away] = [1]\\n\\n# Set the specific teams for this game\\n\\n# Convert new game data into a DataFrame\\nnew_game_df = pd.DataFrame(input_data)\\n\\n# Reorder columns to match the training data\\nnew_game_df = new_game_df[features]\\n\\n# Predict the outcome for the new game\\nprediction = model.predict(new_game_df)\\npredicted_class = (prediction > 0.5).astype(int)\\n\\n# Output the prediction\\nif predicted_class[0][0] == 1:\\n    print(\"The model predicts the home team {} will win.\".format(HOME))\\nelse:\\n    print(\"The model predicts the away team {} will win.\".format(AWAY))\\n'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "# Define features and target\n",
        "features = [col for col in df.columns if col != 'Home Win']\n",
        "target = 'Home Win'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the neural network model\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=X_train.shape[1], activation='relu'),  # Input and first hidden layer\n",
        "    Dense(32, activation='relu'),  # Second hidden layer\n",
        "    Dense(1, activation='sigmoid')  # Output layer (binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=8, verbose=1, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Predict for a new game\n",
        "home_ratings_30_days = RATINGS_30_DAY[HOME]\n",
        "away_ratings_30_days = RATINGS_30_DAY[AWAY]\n",
        "\n",
        "home_ratings_15_days = RATINGS_15_DAY[HOME]\n",
        "away_ratings_15_days = RATINGS_15_DAY[AWAY]\n",
        "\n",
        "home_ratings_7_days = RATINGS_7_DAY[HOME]\n",
        "away_ratings_7_days = RATINGS_7_DAY[AWAY]\n",
        "\n",
        "home_injuries = scrape_team_injuries(HOME)\n",
        "away_injuries = scrape_team_injuries(AWAY)\n",
        "\n",
        "home_injury_value = get_injury_advanced(home_injuries, HOME)\n",
        "away_injury_value = get_injury_advanced(away_injuries, AWAY)\n",
        "\n",
        "\n",
        "input_data = { \n",
        "    \"30 Day Home Net Rating\": [home_ratings_30_days[0] - home_ratings_30_days[1]], \n",
        "    \"30 Day Away Net Rating\": [away_ratings_30_days[0] - away_ratings_30_days[1]], \n",
        "    \"15 Day Home Net Rating\": [home_ratings_15_days[0] - home_ratings_15_days[1]], \n",
        "    \"15 Day Away Net Rating\": [away_ratings_15_days[0] - away_ratings_15_days[1]],\n",
        "    \"7 Day Home Net Rating\": [home_ratings_7_days[0] - home_ratings_7_days[1]], \n",
        "    \"7 Day Away Net Rating\": [away_ratings_7_days[0] - away_ratings_7_days[1]],\n",
        "    \"Home Injury Advanced\": [home_injury_value], \n",
        "    \"Away Injury Advanced\": [away_injury_value],\n",
        "}\n",
        "\n",
        "\n",
        "for col in X.columns:\n",
        "    if col.startswith(\"Home_\") or col.startswith(\"Away_\"):\n",
        "        input_data[col] = [0]\n",
        "\n",
        "home = \"Home_{}\".format(HOME)\n",
        "away = \"Away_{}\".format(AWAY)\n",
        "\n",
        "input_data[home] = [1]\n",
        "input_data[away] = [1]\n",
        "\n",
        "# Set the specific teams for this game\n",
        "\n",
        "# Convert new game data into a DataFrame\n",
        "new_game_df = pd.DataFrame(input_data)\n",
        "\n",
        "# Reorder columns to match the training data\n",
        "new_game_df = new_game_df[features]\n",
        "\n",
        "# Predict the outcome for the new game\n",
        "prediction = model.predict(new_game_df)\n",
        "predicted_class = (prediction > 0.5).astype(int)\n",
        "\n",
        "# Output the prediction\n",
        "if predicted_class[0][0] == 1:\n",
        "    print(\"The model predicts the home team {} will win.\".format(HOME))\n",
        "else:\n",
        "    print(\"The model predicts the away team {} will win.\".format(AWAY))\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
